{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gradient Boosting (GBM) Refer [analyticsvidhya](https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/?utm_source=facebook.com&utm_medium=social&fbclid=IwAR0EgEUYN737p2INzE7gQD8TD3I2BzuIImsE3oTDqdVD-8GEXvxxCGaTTOA)\n",
    "\n",
    "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.\n",
    "\n",
    "We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:\n",
    "\n",
    "    1. The mean age is assumed to be the predicted value for all observations in the dataset.\n",
    "    2. The errors are calculated using this mean prediction and actual values of age.\n",
    "    3. A tree model is created using the errors calculated above as target variable. Our objective is to find the best split to minimize the error.\n",
    "    4. The predictions by this model are combined with the predictions 1.\n",
    "    5. This value calculated above is the new prediction.\n",
    "    6. New errors are calculated using this predicted value and actual value.\n",
    "    7. Steps 2 to 6 are repeated till the maximum number of iterations is reached (or error function does not change).\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Parameters\n",
    "\n",
    "    min_samples_split\n",
    "        Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting.\n",
    "        Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "    min_samples_leaf\n",
    "        Defines the minimum samples required in a terminal or leaf node.\n",
    "        Generally, lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in the majority will be very small.\n",
    "\n",
    "    min_weight_fraction_leaf\n",
    "        Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.\n",
    "    max_depth\n",
    "        The maximum depth of a tree.\n",
    "        Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample.\n",
    "        Should be tuned using CV.\n",
    "    max_leaf_nodes\n",
    "        The maximum number of terminal nodes or leaves in a tree.\n",
    "        Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "        If this is defined, GBM will ignore max_depth.\n",
    "    max_features\n",
    "        The number of features to consider while searching for the best split. These will be randomly selected.\n",
    "        As a thumb-rule, the square root of the total number of features works great but we should check up to 30-40% of the total number of features.\n",
    "        Higher values can lead to over-fitting but it generally depends on a case to case scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
